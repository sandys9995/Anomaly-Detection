{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df61813",
   "metadata": {},
   "source": [
    " **Content**\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "2. [The following are the important libraries](#The-following-are-the-important-libraries)\n",
    "\n",
    "   2.1. [Correct Data for Comparison](#Correct-Data-for-Comparison)\n",
    "   \n",
    "   2.2. [Reading JSON Data](#Reading-JSON-Data)\n",
    "   \n",
    "3. [Dashboard for Predictive Maintenance](#Dashboard-for-Predictive-Maintenance)\n",
    "\n",
    "   3.1. [Initialization](#Initialization)\n",
    "   \n",
    "   3.2. [File Upload Section](#File-Upload-Section)\n",
    "   \n",
    "   3.3. [Data Processing](#Data-Processing)\n",
    "   \n",
    "   3.4. [Data Display](#Data-Display)\n",
    "   \n",
    "   3.5. [Test Experiment](#Test-Experiment)\n",
    "   \n",
    "   3.6. [Data Drift Experiment](#Data-Drift-Experiment)\n",
    "   \n",
    "   3.7. [Confusion Matrix](#Confusion-Matrix)\n",
    "   \n",
    "   3.8. [Correlation Matrix](#Correlation-Matrix)\n",
    "   \n",
    "   3.9. [Error Handling](#Error-Handling)\n",
    "   \n",
    "   3.10. [Main Execution](#Main-Execution)\n",
    "\n",
    "\n",
    "You can use these clickable section titles to navigate to specific sections in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9ba6a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "This notebook serves as a comprehensive Predictive Maintenance Dashboard, allowing users to interact with the results of predictive maintenance analyses. It utilizes a Streamlit web application to provide an intuitive and user-friendly interface for exploring machine health data and predictive models.\n",
    "\n",
    "The primary objective of this dashboard is to enable users to:\n",
    "\n",
    "- Upload a CSV file containing machine data.\n",
    "- Process the uploaded data using a FastAPI backend.\n",
    "- Visualize and analyze the results of predictive maintenance experiments.\n",
    "- Explore machine health indicators and their impact.\n",
    "\n",
    "The dashboard offers several features, including data visualization, experiment selection, and a \"Run\" button to trigger the analysis process. It allows users to choose different types of experiments, such as \"Test,\" \"Data Drift,\" \"Confusion Matrix,\" and \"Correlation Matrix,\" providing a versatile platform for predictive maintenance tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6a077",
   "metadata": {},
   "source": [
    "# The following are the important libraries\n",
    "\n",
    "............................................................................................................................................................................................................\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2d8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e720ff",
   "metadata": {},
   "source": [
    "## Correct Data for Comparison\n",
    "This section loads and prepares the data for comparison and analysis. It consists of the following steps:\n",
    "\n",
    "1. Reading and preprocessing the CSV data file.\n",
    "2. Detecting peaks and valleys in the signal data.\n",
    "3. Data filtering and segmentation.\n",
    "4. Grouping the data into experiments based on detected peaks and valleys.\n",
    "5. Filtering the grouped data based on specific conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data_dir= \"2020-11-25_09-10-56-DTT-MILLP500U-X.csv\"\n",
    "\n",
    "column_indices = [0, 1, 2, 4, 6, 8, 9]\n",
    "expected_columns = ['Time','PosDiff/X', 'v act/X', 's act/X','a act/X', 'I (nom)/X', 's diff/X']\n",
    "initial_time = -464151000.0\n",
    "\n",
    "df_corr = pd.read_csv(corr_data_dir,delimiter=\";\", usecols=column_indices)\n",
    "df_corr.columns= expected_columns\n",
    "df_corr = df_corr[df_corr['Time'] >= initial_time].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "peaks, _ = find_peaks(df_corr.iloc[:,3])\n",
    "#peaks_idx.append(peaks)\n",
    "valleys, _ = find_peaks(-df_corr.iloc[:,3])  # Find valleys by negating the signal data\n",
    "#valleys_idx.append(valleys)\n",
    "\n",
    "a= len(df_corr.iloc[:,3][valleys][(df_corr.iloc[:,3][valleys] > 50) & (df_corr.iloc[:,3][valleys] < 600) & (df_corr.iloc[:,3][valleys].index<20000)].index)\n",
    "b= len(df_corr.iloc[:,3][peaks][(df_corr.iloc[:,3][peaks] > 500) & (df_corr.iloc[:,3][peaks] < 600) & (df_corr.iloc[:,3][peaks].index>20000) & (df_corr.iloc[:,3][peaks].index<80000)].index) \n",
    "\n",
    "warm_len= a + b\n",
    "\n",
    "start_point = warm_len\n",
    "end_point = peaks.shape[0]\n",
    "\n",
    "# Calculate the step size based on the number of steps (3 in this case)\n",
    "step_size = 3\n",
    "# Create the array with multiples of 3 till 15 times\n",
    "req_segment= np.arange(warm_len, end_point,step_size)\n",
    "\n",
    "\n",
    "exp_corr = []\n",
    "\n",
    "for j in range(0,5):\n",
    "    exp_corr.append(pd.DataFrame(df_corr[valleys[req_segment[j]]: valleys[req_segment[j+1]]]).reset_index(drop= True))\n",
    "for j in range(5,10):\n",
    "    exp_corr.append(pd.DataFrame(df_corr[valleys[req_segment[j]]: valleys[req_segment[j+1]]]).reset_index(drop=True))\n",
    "for j  in range(10,15):\n",
    "    exp_corr.append(pd.DataFrame(df_corr[peaks[req_segment[j]]: peaks[req_segment[j+1]]]).reset_index(drop=True))\n",
    "\n",
    "\n",
    "exp_corr1_1= (exp_corr[0][exp_corr[0].iloc[:,3]> 332].reset_index(drop= True))\n",
    "exp_corr1_2= (exp_corr[1][exp_corr[1].iloc[:,3]>323])\n",
    "exp_corr1_3= (exp_corr[2][exp_corr[2].iloc[:,3]>310])\n",
    "exp_corr1_4= (exp_corr[3])\n",
    "exp_corr1_5= (exp_corr[4][exp_corr[4].iloc[:,3]>250])\n",
    "\n",
    "exp_corr_1 = [exp_corr1_1, exp_corr1_2, exp_corr1_3, exp_corr1_4, exp_corr1_5] \n",
    "\n",
    "exp_corr_2= []\n",
    "exp_corr_3= []\n",
    "\n",
    "for i in range(5,15):\n",
    "    if i < 10:\n",
    "        exp_corr_2.append(exp_corr[i])\n",
    "    else :\n",
    "        exp_corr_3.append(exp_corr[i])\n",
    "\n",
    "experiments_corr= [exp_corr_1,exp_corr_2,exp_corr_3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf8aeb",
   "metadata": {},
   "source": [
    "\n",
    "## Reading JSON Data\n",
    "This section covers the process of reading JSON data from files. It includes the following steps:\n",
    "\n",
    "1. Defining a function to read JSON files.\n",
    "2. Specifying the root directory where JSON files are stored.\n",
    "3. Iterating through files and directories to collect JSON data.\n",
    "4. Sorting the collected files based on location and file name.\n",
    "5. Creating a list of defective indexes.\n",
    "6. Reading and collecting experiments from JSON files.\n",
    "\n",
    "The provided code structures and processes the data for further analysis and comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        data = pd.read_json(file_path).reset_index(drop=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred while reading JSON data from {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "rootdir = \"/GM_usecase/backend_fastapi/app/faulty_data\"\n",
    "\n",
    "all_faults = []\n",
    "location = []\n",
    "\n",
    "for root, directories, files in os.walk(rootdir):\n",
    "    for file_name in files:\n",
    "        location.append(root)\n",
    "        all_faults.append(file_name)\n",
    "\n",
    "def sorting_location(path):\n",
    "    # Split the path by '_' and get the last part\n",
    "    parts = path.split('_')\n",
    "    # Convert the last part to an integer for sorting\n",
    "    return int(parts[-1])\n",
    "\n",
    "def sorting_file_name(path):\n",
    "    # Split the path by '_' and get the last part\n",
    "    parts = (path.replace(\".\",\"_\")).split(\"_\")\n",
    "    # Convert the last part to an integer for sorting\n",
    "    return int(parts[-2])\n",
    "\n",
    "# Sort the file_paths based on the end number\n",
    "sorted_loc = sorted(location, key=sorting_location)\n",
    "sorted_file_name = sorted(all_faults, key=sorting_file_name)\n",
    "\n",
    "loc_faulty_signal = [os.path.join(x, y) for x, y in zip(sorted_loc, sorted_file_name)]\n",
    "\n",
    "defective_index = []\n",
    "experiments = []\n",
    "\n",
    "for i, file_path in enumerate(loc_faulty_signal):\n",
    "    defective_index.append(sorting_location(location[i]))\n",
    "    defective_index = sorted(defective_index)\n",
    "    data = read_json_file(file_path)\n",
    "    if data is not None:\n",
    "        experiments.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4047257",
   "metadata": {},
   "source": [
    "# Dashboard for Predictive Maintenance\n",
    "This section sets up a Streamlit dashboard for predictive maintenance. It allows users to upload a CSV file, process it, and interact with the results. The code description includes the following:\n",
    "\n",
    "## Initialization\n",
    "- Initializes the Streamlit app and session state.\n",
    "- Creates the main title and subheader for the dashboard.\n",
    "\n",
    "## File Upload Section\n",
    "- Allows users to upload a CSV file.\n",
    "- Checks if a file is uploaded and shows a processing message.\n",
    "\n",
    "## Data Processing\n",
    "- Sends an HTTP POST request to a FastAPI endpoint to process the uploaded CSV file.\n",
    "- Handles responses from FastAPI.\n",
    "- Validates the response status code and displays relevant information.\n",
    "- Parses the uploaded file name to extract machine type, date, and axis.\n",
    "- Creates widgets for user interaction in the sidebar, including experiment selection, machine type, date, and axis.\n",
    "- Provides a \"Run\" button to load new results.\n",
    "\n",
    "## Data Display\n",
    "- Displays the selected options, including experiment type.\n",
    "- Provides a classification of defect severity.\n",
    "- Allows users to select a signal type (e.g., 'PosDiff/X', 'v act/X') to display.\n",
    "\n",
    "## Test Experiment\n",
    "- If 'Test' experiment is selected, iterates through defective indexes and displays line charts for each.\n",
    "\n",
    "## Data Drift Experiment\n",
    "- If 'Data Drift' experiment is selected, parses and displays the response data as a DataFrame.\n",
    "\n",
    "## Confusion Matrix\n",
    "- If 'Confusion Matrix' is selected, displays an image of a confusion matrix.\n",
    "\n",
    "## Correlation Matrix\n",
    "- If 'Correlation Matrix' is selected, displays an image of a correlation matrix.\n",
    "\n",
    "## Error Handling\n",
    "- Provides error messages for any issues that occur during file upload, processing, or plotting.\n",
    "\n",
    "## Main Execution\n",
    "- The code execution begins when the script is run.\n",
    "\n",
    "This code sets up a Streamlit dashboard for interacting with predictive maintenance results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec01a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    if 'run_button_state' not in st.session_state:\n",
    "        st.session_state.run_button_state = False\n",
    "\n",
    "    st.title('Dashboard')\n",
    "    st.subheader('Predictive Maintenance')\n",
    "\n",
    "    # File Upload Section\n",
    "    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=[\"csv\"])\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        st.write(\"Processing the uploaded file...\")\n",
    "    else:\n",
    "        st.write(\"Upload a CSV file to process.\")\n",
    "\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Add the retry mechanism\n",
    "            retry_strategy = Retry(\n",
    "                total=5,\n",
    "                backoff_factor=2,\n",
    "                status_forcelist=[429, 500, 502, 503, 504],\n",
    "            )\n",
    "            adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "            http = requests.Session()\n",
    "            http.mount(\"http://\", adapter)\n",
    "\n",
    "            # Make an HTTP POST request to your FastAPI endpoint\n",
    "            api_endpoint = \"http://fastapi:80/predict/\"\n",
    "            \n",
    "            # Prepare the file for upload\n",
    "            files = {'file': (uploaded_file)}\n",
    "            \n",
    "            response = requests.post(api_endpoint, files=files)\n",
    "            \n",
    "            if (response.status_code == 201) | (response.status_code==200) :\n",
    "                #st.write(\"Response from FastAPI:\")\n",
    "                \n",
    "                # Once the file upload and response are complete, you can display your charts here.\n",
    "                # Create widgets for user interaction\n",
    "                \n",
    "                uploaded_file_name= uploaded_file.name\n",
    "\n",
    "                substrings_to_check = [\"MILLP500\", \"MillP500\", \"Mill_P500\"]\n",
    "                date = uploaded_file_name.split('_')[0]\n",
    "                axis= uploaded_file_name.replace('.',\"-\").split('-')[-2]\n",
    "\n",
    "                # Check if any of the substrings is present in the string\n",
    "                found_substring = any(substring in uploaded_file_name for substring in substrings_to_check)\n",
    "\n",
    "                if found_substring:\n",
    "                    Machine_type= \"MILL P 500 U\"\n",
    "                else:\n",
    "                    Machine_type= 'None'\n",
    "                \n",
    "        \n",
    "                # Create widgets for user interaction in the sidebar\n",
    "                with st.sidebar:\n",
    "                    experiment_selector = st.radio('Select Experiment', ['Test','Data Drift','Confusion Matrix',\"Correlation Matrix\"])\n",
    "                    selected_machine_type = st.radio('Machine Type', [Machine_type])\n",
    "                    selected_date = st.radio('Date', [date])\n",
    "                    selected_axis = st.radio('Axis', [axis])\n",
    "                    st.write(\"NOTE: Please Share The 'Feedback'\")\n",
    "                    \n",
    "                ### run button to load result\n",
    "                run_button = st.button('Run')\n",
    "                new_title = '<p style=\"font-family:sans-serif; color:Black; font-size: 12px;\">Please Press \"RUN\" to load the new results</p>'\n",
    "                st.markdown(new_title, unsafe_allow_html=True)\n",
    "\n",
    "                # Display the selected options\n",
    "                st.write(\"Type:\", experiment_selector)\n",
    "                \n",
    "                columns = {'PosDiff/X': 1, 'v act/X': 2, 's act/X': 3, 'a act/X': 4, 'I (nom)/X': 5, 's diff/X': 6}\n",
    "\n",
    "\n",
    "                # Loop through defective indexes and display plots based on user selections\n",
    "                if experiment_selector == 'Test':\n",
    "                    def classify_defect(x):\n",
    "                        if x <= 4:\n",
    "                            return \"Low Critical\"\n",
    "                        elif 5 <= x < 10:\n",
    "                            return \"Critical\"\n",
    "                        else:\n",
    "                            return \"High Critical\"\n",
    "                    \n",
    "                    st.subheader(f'The total number of Defect: {len(defective_index)} and Health Indicator: {classify_defect(len(defective_index))}')\n",
    "                        \n",
    "                    selected_col = st.selectbox('Signal Type', columns.keys())\n",
    "\n",
    "                    for i, indx in enumerate(defective_index):\n",
    "                        try:\n",
    "                            data = experiments[i].iloc[:, columns[selected_col]]\n",
    "                            data1 = experiments_corr[indx // 5][indx % 5].iloc[:, columns[selected_col]]\n",
    "\n",
    "                            # Create a DataFrame for this iteration's data\n",
    "                            df = pd.DataFrame({'Test': data, 'Corr': data1})\n",
    "\n",
    "                            # Display the plot for this iteration\n",
    "                            st.subheader(f'Defective Index {indx}, Experiment {experiment_selector}')\n",
    "                            st.line_chart(df, use_container_width=True)\n",
    "                        except Exception as e:\n",
    "                            st.error(f\"An error occurred while plotting: {str(e)}\")\n",
    "\n",
    "                elif experiment_selector == 'Data Drift':\n",
    "                        response_dict = json.loads(response.text)\n",
    "\n",
    "                        # Now response_dict is a Python dictionary\n",
    "                        st.write(pd.DataFrame(list(response_dict.items())[1][1]))\n",
    "                    \n",
    "                elif experiment_selector == 'Confusion Matrix':\n",
    "                    # Your image file path\n",
    "                    image_path = f\"{BASE_DIR}/Confusion_Matrix.png\"\n",
    "                    st.image(image_path,channels=\"BGR\")\n",
    "\n",
    "                else :\n",
    "                    # Your image file path\n",
    "                    image_path = f\"{BASE_DIR}/correlation_matrix.png\"\n",
    "                    st.image(image_path,channels=\"BGR\")\n",
    "\n",
    "                \n",
    "            else:\n",
    "                st.error(f\"Failed to upload file. Status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred while uploading the file: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfsandy",
   "language": "python",
   "name": "tfsandy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "294.86px",
    "width": "371.877px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
